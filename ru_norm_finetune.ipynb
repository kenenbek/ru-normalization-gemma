{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b2f2a4-8796-456d-b16d-317c374df466",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-3-270m-it\" # @param [\"google/gemma-3-270m-it\",\"google/gemma-3-1b-it\",\"google/gemma-3-4b-it\",\"google/gemma-3-12b-it\",\"google/gemma-3-27b-it\"] {\"allow-input\":true}\n",
    "checkpoint_dir = \"/mnt/d/ru_norm_gemma\"\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7817fdc4-7c70-459a-9545-5c2a2a22b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'После окончания японо - китайской войны Хуан Цзунъин получила приглашение от одной из шанхайских кинокомпаний сняться в картине « Преследование ».', 'role': 'user'}, {'content': 'после окончания японо  китайской войны хуан цзунъин получила приглашение от одной из шанхайских кинокомпаний сняться в картине  преследование .', 'role': 'assistant'}]\n",
      "[{'content': '106. Чермоев Абдул Меджид Арцуевич / / БСЭ.', 'role': 'user'}, {'content': 'сто шесть. чермоев абдул меджид арцуевич   бсэ.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the Hub\n",
    "dataset = load_dataset(\"kenenbek/gemma-russian-normalization-dataset\")\n",
    "print(dataset[\"train\"][0][\"messages\"])\n",
    "print(dataset[\"validation\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2717267a-8fa6-452b-8b0c-02850af83f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "DType: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90960198-cb2b-4406-af65-cb485cb54497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,              # directory to save and repository id\n",
    "    max_length=512,                         # max sequence length for model and packing of the dataset\n",
    "    packing=False,                          # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=5,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    gradient_checkpointing=False,           # Caching is incompatible with gradient checkpointing\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                        # log every step\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                  # evaluate checkpoint every epoch\n",
    "    learning_rate=learning_rate,            # learning rate\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    report_to=\"wandb\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # Template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6527a8d8-454e-437c-98eb-103ae8c372f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2f997-49bb-408d-9ecf-9ce019050129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='671535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    11/671535 00:01 < 28:41:58, 6.50 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3639' max='7462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3639/7462 07:59 < 08:23, 7.59 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4433eab-7e91-4f5d-b9ed-e5f4b68d8123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968e29f1-a670-4406-a76b-752d92997569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "         TOKEN CONFIGURATION OVERVIEW\n",
      "========================================\n",
      "\n",
      "--- 1. Tokenizer ---\n",
      "BOS token:      '<bos>' (ID: 2)\n",
      "EOS token:      '<eos>' (ID: 1)\n",
      "PAD token:      '<pad>' (ID: 0)\n",
      "UNK token:      '<unk>' (ID: 3)\n",
      "----------------------------------------\n",
      "--- 2. Model Config (`model.config`) ---\n",
      "BOS token ID:   2\n",
      "EOS token ID:   1\n",
      "PAD token ID:   0\n",
      "----------------------------------------\n",
      "--- 3. Generation Config (`model.generation_config`) ---\n",
      "BOS token ID:   2\n",
      "EOS token ID:   [1, 106]\n",
      "PAD token ID:   0\n",
      "========================================\n",
      "\n",
      "✅ All configurations are aligned.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer does not have a pad_token. Setting it to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Important: Update the model's config to reflect this change\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# --- Print All Configurations for Comparison ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"         TOKEN CONFIGURATION OVERVIEW\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# 1. Tokenizer\n",
    "print(\"--- 1. Tokenizer ---\")\n",
    "print(f\"{'BOS token:':<15} '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"{'EOS token:':<15} '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"{'PAD token:':<15} '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"{'UNK token:':<15} '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2. Model Config\n",
    "print(\"--- 2. Model Config (`model.config`) ---\")\n",
    "print(f\"{'BOS token ID:':<15} {model.config.bos_token_id}\")\n",
    "print(f\"{'EOS token ID:':<15} {model.config.eos_token_id}\")\n",
    "print(f\"{'PAD token ID:':<15} {model.config.pad_token_id}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3. Generation Config\n",
    "print(\"--- 3. Generation Config (`model.generation_config`) ---\")\n",
    "print(f\"{'BOS token ID:':<15} {model.generation_config.bos_token_id}\")\n",
    "print(f\"{'EOS token ID:':<15} {model.generation_config.eos_token_id}\")\n",
    "print(f\"{'PAD token ID:':<15} {model.generation_config.pad_token_id}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- Final Check for Alignment ---\n",
    "if (tokenizer.pad_token_id == model.config.pad_token_id and\n",
    "    tokenizer.eos_token_id == model.config.eos_token_id and\n",
    "    tokenizer.bos_token_id == model.config.bos_token_id):\n",
    "    print(\"\\n✅ All configurations are aligned.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Warning: Mismatch detected between tokenizer and model configs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485eb198-ba77-48e3-8510-2d747ee3ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with ID 106 is: '<end_of_turn>'\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' is your loaded tokenizer object\n",
    "secondary_stop_token_id = 106\n",
    "decoded_token = tokenizer.decode([secondary_stop_token_id])\n",
    "\n",
    "print(f\"The token with ID {secondary_stop_token_id} is: '{decoded_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a7923-be2e-4296-a21c-ebd4777f83e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
