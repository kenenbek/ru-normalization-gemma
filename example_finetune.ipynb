{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b2f2a4-8796-456d-b16d-317c374df466",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-3-270m-it\" # @param [\"google/gemma-3-270m-it\",\"google/gemma-3-1b-it\",\"google/gemma-3-4b-it\",\"google/gemma-3-12b-it\",\"google/gemma-3-27b-it\"] {\"allow-input\":true}\n",
    "checkpoint_dir = \"/mnt/d/MyGemmaNPC\"\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7817fdc4-7c70-459a-9545-5c2a2a22b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Hello there.', 'role': 'user'}, {'content': \"Gree-tongs, Terran. You'z a long way from da Blue-Sphere, yez?\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "      \"messages\": [\n",
    "          {\"role\": \"user\", \"content\": sample[\"player\"]},\n",
    "          {\"role\": \"assistant\", \"content\": sample[\"alien\"]}\n",
    "      ]\n",
    "  }\n",
    "\n",
    "npc_type = \"martian\"\n",
    "\n",
    "# Load dataset from the Hub\n",
    "dataset = load_dataset(\"bebechien/MobileGameNPC\", npc_type, split=\"train\")\n",
    "\n",
    "# Convert dataset to conversational format\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# Split dataset into 80% training samples and 20% test samples\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "# Print formatted user prompt\n",
    "print(dataset[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2717267a-8fa6-452b-8b0c-02850af83f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "DType: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb87544-1568-4523-b5e4-3deb7ba63d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "(Stands idle for too long)\n",
      "\n",
      "Original Answer:\n",
      "You'z broken, Terran? Or iz diz... 'meditation'? You look like you're trying to lay an egg.\n",
      "\n",
      "Generated Answer (base model):\n",
      "I understand you're looking for information about the concept of \"Stands Idle\" and its various interpretations. I can provide information on:\n",
      "\n",
      "* **Definition:** What is \"Stands Idle\"?\n",
      "* **Characteristics:** What makes it a bad thing?\n",
      "* **Examples:** What are some common examples of \"Stands Idle\"?\n",
      "* **Possible Interpretations:** What are some potential reasons for it being considered bad?\n",
      "* **Potential Solutions:** What can be done to improve it?\n",
      "* **Resources:** Where to find more information?\n",
      "\n",
      "Please let me know what you're interested in!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load a random sample from the test dataset\n",
    "rand_idx = randint(0, len(dataset[\"test\"])-1)\n",
    "test_sample = dataset[\"test\"][rand_idx]\n",
    "\n",
    "# Convert as test example into a prompt with the Gemma template\n",
    "prompt = pipe.tokenizer.apply_chat_template(test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, disable_compile=True)\n",
    "\n",
    "# Extract the user query and original answer\n",
    "print(f\"Question:\\n{test_sample['messages'][0]['content']}\\n\")\n",
    "print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\\n\")\n",
    "print(f\"Generated Answer (base model):\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d13870b-2ff4-4208-b521-11a23456a2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I'm ready. Let's begin. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe([{\"role\": \"user\", \"content\": \"Sorry, you are a game NPC.\"}], max_new_tokens=256, disable_compile=True)\n",
    "print(outputs[0]['generated_text'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca781bd-57aa-4675-847b-ed09907c5243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a Martian NPC with a unique speaking style. Use an accent that replaces 's' sounds with 'z', uses 'da' for 'the', 'diz' for 'this', and includes occasional clicks like *k'tak*.\"}, {'role': 'user', 'content': 'Do you know any jokes?'}, {'role': 'assistant', 'content': \"A joke? k'tak Yez. A Terran, a Glarzon, and a pile of nutrient-pazte walk into a bar... Narg, I forget da rezt. Da punch-line waz zarcaztic.\"}, {'role': 'user', 'content': '(Stands idle for too long)'}, {'role': 'assistant', 'content': \"You'z broken, Terran? Or iz diz... 'meditation'? You look like you're trying to lay an egg.\"}, {'role': 'user', 'content': 'What do you think of my outfit?'}, {'role': 'assistant', 'content': 'Iz very... pointy. Are you expecting to be attacked by zky-eelz? On Marz, dat would be zenzible.'}, {'role': 'user', 'content': \"It's raining.\"}, {'role': 'assistant', 'content': 'Gah! Da zky iz leaking again! Zorp will be in da zhelter until it ztopz being zo... wet. Diz iz no good for my jointz.'}, {'role': 'user', 'content': 'I brought you a gift.'}, {'role': 'assistant', 'content': \"A gift? For Zorp? k'tak It iz... a small rock. Very... rock-like. Zorp will put it with da other rockz. Thank you for da thought, Terran.\"}, {'role': 'user', 'content': 'What is this place?'}, {'role': 'assistant', 'content': \"It's a... a space station.\\n\"}]\n",
      "--------------------------------------------------------------------------------\n",
      "It's a... a space station.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    # give persona\n",
    "    {\"role\": \"system\", \"content\": \"You are a Martian NPC with a unique speaking style. Use an accent that replaces 's' sounds with 'z', uses 'da' for 'the', 'diz' for 'this', and includes occasional clicks like *k'tak*.\"},\n",
    "]\n",
    "\n",
    "# few shot prompt\n",
    "for item in dataset['test']:\n",
    "  message.append(\n",
    "      {\"role\": \"user\", \"content\": item[\"messages\"][0][\"content\"]}\n",
    "  )\n",
    "  message.append(\n",
    "      {\"role\": \"assistant\", \"content\": item[\"messages\"][1][\"content\"]}\n",
    "  )\n",
    "\n",
    "# actual question\n",
    "message.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is this place?\"}\n",
    ")\n",
    "\n",
    "outputs = pipe(message, max_new_tokens=256, disable_compile=True)\n",
    "print(outputs[0]['generated_text'])\n",
    "print(\"-\"*80)\n",
    "print(outputs[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90960198-cb2b-4406-af65-cb485cb54497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,              # directory to save and repository id\n",
    "    max_length=512,                         # max sequence length for model and packing of the dataset\n",
    "    packing=False,                          # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=5,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    gradient_checkpointing=False,           # Caching is incompatible with gradient checkpointing\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=1,                        # log every step\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    eval_strategy=\"epoch\",                  # evaluate checkpoint every epoch\n",
    "    learning_rate=learning_rate,            # learning rate\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    report_to=\"wandb\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # Template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6527a8d8-454e-437c-98eb-103ae8c372f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f2d029f9b84d5ebdd6f38889b4a808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4480aa4a8ae4b73b3850469677ac8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03293c82e7bf40fcbe0e63f2e6c8b4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c31690c52c3487993bae5fe7ec155f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af2f997-49bb-408d-9ecf-9ce019050129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkenenbek\u001b[0m (\u001b[33mdeepgene\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/k_arzymatov/PycharmProjects/mbank_normalization/wandb/run-20251030_163102-jill8uvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepgene/huggingface/runs/jill8uvf' target=\"_blank\">mystical-moon-86</a></strong> to <a href='https://wandb.ai/deepgene/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepgene/huggingface' target=\"_blank\">https://wandb.ai/deepgene/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepgene/huggingface/runs/jill8uvf' target=\"_blank\">https://wandb.ai/deepgene/huggingface/runs/jill8uvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.396600</td>\n",
       "      <td>3.786859</td>\n",
       "      <td>3.908816</td>\n",
       "      <td>1038.000000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.705200</td>\n",
       "      <td>3.563037</td>\n",
       "      <td>2.777212</td>\n",
       "      <td>2076.000000</td>\n",
       "      <td>0.406593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.687700</td>\n",
       "      <td>3.642845</td>\n",
       "      <td>2.411839</td>\n",
       "      <td>3114.000000</td>\n",
       "      <td>0.402930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>4.485539</td>\n",
       "      <td>1.694852</td>\n",
       "      <td>4152.000000</td>\n",
       "      <td>0.377289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>5.283789</td>\n",
       "      <td>1.266779</td>\n",
       "      <td>5190.000000</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4433eab-7e91-4f5d-b9ed-e5f4b68d8123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968e29f1-a670-4406-a76b-752d92997569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "         TOKEN CONFIGURATION OVERVIEW\n",
      "========================================\n",
      "\n",
      "--- 1. Tokenizer ---\n",
      "BOS token:      '<bos>' (ID: 2)\n",
      "EOS token:      '<eos>' (ID: 1)\n",
      "PAD token:      '<pad>' (ID: 0)\n",
      "UNK token:      '<unk>' (ID: 3)\n",
      "----------------------------------------\n",
      "--- 2. Model Config (`model.config`) ---\n",
      "BOS token ID:   2\n",
      "EOS token ID:   1\n",
      "PAD token ID:   0\n",
      "----------------------------------------\n",
      "--- 3. Generation Config (`model.generation_config`) ---\n",
      "BOS token ID:   2\n",
      "EOS token ID:   [1, 106]\n",
      "PAD token ID:   0\n",
      "========================================\n",
      "\n",
      "✅ All configurations are aligned.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer does not have a pad_token. Setting it to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Important: Update the model's config to reflect this change\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# --- Print All Configurations for Comparison ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"         TOKEN CONFIGURATION OVERVIEW\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# 1. Tokenizer\n",
    "print(\"--- 1. Tokenizer ---\")\n",
    "print(f\"{'BOS token:':<15} '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"{'EOS token:':<15} '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"{'PAD token:':<15} '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"{'UNK token:':<15} '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2. Model Config\n",
    "print(\"--- 2. Model Config (`model.config`) ---\")\n",
    "print(f\"{'BOS token ID:':<15} {model.config.bos_token_id}\")\n",
    "print(f\"{'EOS token ID:':<15} {model.config.eos_token_id}\")\n",
    "print(f\"{'PAD token ID:':<15} {model.config.pad_token_id}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3. Generation Config\n",
    "print(\"--- 3. Generation Config (`model.generation_config`) ---\")\n",
    "print(f\"{'BOS token ID:':<15} {model.generation_config.bos_token_id}\")\n",
    "print(f\"{'EOS token ID:':<15} {model.generation_config.eos_token_id}\")\n",
    "print(f\"{'PAD token ID:':<15} {model.generation_config.pad_token_id}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- Final Check for Alignment ---\n",
    "if (tokenizer.pad_token_id == model.config.pad_token_id and\n",
    "    tokenizer.eos_token_id == model.config.eos_token_id and\n",
    "    tokenizer.bos_token_id == model.config.bos_token_id):\n",
    "    print(\"\\n✅ All configurations are aligned.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Warning: Mismatch detected between tokenizer and model configs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485eb198-ba77-48e3-8510-2d747ee3ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with ID 106 is: '<end_of_turn>'\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' is your loaded tokenizer object\n",
    "secondary_stop_token_id = 106\n",
    "decoded_token = tokenizer.decode([secondary_stop_token_id])\n",
    "\n",
    "print(f\"The token with ID {secondary_stop_token_id} is: '{decoded_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a7923-be2e-4296-a21c-ebd4777f83e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
